{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI9KZ3F8TLSK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# EfficientDet Training On ASL Dataset\n",
    "---\n",
    " \n",
    "## Set up on Nvidia Jetson NX\n",
    "\n",
    "- Using Docker Image Inference which uses the base image nvcr.io/nvidia/l4t-pytorch:r32.6.1-pth1.9-py3 from the Dockerfile.yolov5\n",
    "- Building Docker image\n",
    "\n",
    "```bash\n",
    "docker build -t yolov5 -f Dockerfile.yolov5 . \n",
    "```\n",
    "- Run the container and mounting data in the /app/ folder\n",
    "\n",
    "```bash\n",
    "docker run -ti --rm --runtime nvidia  --device /dev/video0 --network host --privileged -e DISPLAY=$DISPLAY -v /data/w251:/app/w251 yolov5\n",
    "```\n",
    "- Run the following line before spinning up Jupyter to update Jupyter config\n",
    "\n",
    "```bash\n",
    "export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67-3S5_VTLSL",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### 0. Install Requirements & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download and upzip annotated data from roboflow\n",
    "!curl -L \"https://app.roboflow.com/ds/3ZcnERAu61?key=8mQAZkmZt5\" -o roboflow.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p Yet-Another-EfficientDet-Pytorch/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p Yet-Another-EfficientDet-Pytorch/datasets/asl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip roboflow.zip -d Yet-Another-EfficientDet-Pytorch/datasets/asl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm roboflow.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if \"projects\" not in os.getcwd():\n",
    "  os.chdir('Yet-Another-EfficientDet-Pytorch')\n",
    "  sys.path.append('.')\n",
    "else:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv datasets/asl/valid datasets/asl/val\n",
    "!mkdir datasets/asl/annotations\n",
    "!mv datasets/asl/train/_annotations.coco.json datasets/asl/annotations/instances_train.json\n",
    "!mv datasets/asl/test/_annotations.coco.json datasets/asl/annotations/instances_test.json\n",
    "!mv datasets/asl/val/_annotations.coco.json datasets/asl/annotations/instances_val.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-R5C4DaETLSS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Prepare Custom Dataset/Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmCQj3rhTLSS",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download pretrained weights\n",
    "! mkdir weights\n",
    "#! wget https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch/releases/download/1.0/efficientdet-d1.pth -O weights/efficientdet-d1.pth\n",
    "\n",
    "# prepare project file projects/asl.yml\n",
    "# showing its contents here\n",
    "! cat projects/asl.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate & Inference on Nvidia Jetson NX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get latest weight file\n",
    "%cd logs/asl\n",
    "weight_file = !ls -Art | grep efficientdet\n",
    "%cd ../..\n",
    "\n",
    "print(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weight_file[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with PF32\n",
    "!python3 coco_eval.py -c 0 -p asl -w \"logs/asl/{weight_file[-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Inference with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from backbone import EfficientDetBackbone\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess\n",
    "\n",
    "import pathlib\n",
    "\n",
    "compound_coef = 0\n",
    "force_input_size = None  # set None to use default size\n",
    "\n",
    "imgdir_path = pathlib.Path('datasets/asl/test')\n",
    "# img_path = 'datasets/asl/test/thanks-46_jpg.rf.3e299223f7df6ed6916e0e1d862159ea.jpg'\n",
    "img_paths = [str(path) for path in imgdir_path.glob('*.jpg')]\n",
    "print('Number of pictures in test folder:', len(img_paths))\n",
    "img_paths = img_paths[:50]\n",
    "\n",
    "threshold = 0.2\n",
    "iou_threshold = 0.2\n",
    "\n",
    "use_cuda = True\n",
    "use_float16 =False\n",
    "cudnn.fastest = False\n",
    "cudnn.benchmark = True\n",
    "\n",
    "obj_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
    "            'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'four', \n",
    "            'hello', 'help', 'one', 'right', 'thanks', 'three', 'two', 'zero' ]\n",
    "\n",
    "# tf bilinear interpolation is different from any other's, just make do\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n",
    "input_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n",
    "print('input size:', input_size,'\\n')\n",
    "\n",
    "for idx, img_path in enumerate(img_paths):\n",
    "    print(img_path)\n",
    "    ori_imgs, framed_imgs, framed_metas = preprocess(img_path, max_size=input_size)\n",
    "\n",
    "    if use_cuda:\n",
    "        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "    else:\n",
    "        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "    model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list),\n",
    "\n",
    "    # replace this part with your project's anchor config\n",
    "    ratios=[(1.0, 1.0), (1.3, 0.8), (1.9, 0.5)],\n",
    "    scales=[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    model.load_state_dict(torch.load('logs/asl/'+weight_file[-1]))\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    if use_float16:\n",
    "        model = model.half()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features, regression, classification, anchors = model(x)\n",
    "\n",
    "        regressBoxes = BBoxTransform()\n",
    "        clipBoxes = ClipBoxes()\n",
    "\n",
    "        out = postprocess(x,\n",
    "                          anchors, regression, classification,\n",
    "                          regressBoxes, clipBoxes,\n",
    "                          threshold, iou_threshold)\n",
    "\n",
    "    out = invert_affine(framed_metas, out)\n",
    "\n",
    "    for i in range(len(ori_imgs)):\n",
    "        if len(out[i]['rois']) == 0:\n",
    "            continue\n",
    "        ori_imgs[i] = ori_imgs[i].copy()\n",
    "        scores = []\n",
    "        objs = []\n",
    "        for j in range(len(out[i]['rois'])):\n",
    "    #         (x1, y1, x2, y2) = out[i]['rois'][j].astype(np.int)\n",
    "    #         cv2.rectangle(ori_imgs[i], (x1, y1), (x2, y2), (255, 255, 0), 1)\n",
    "            obj = obj_list[out[i]['class_ids'][j]]\n",
    "            objs.append(obj)\n",
    "            score = float(out[i]['scores'][j])\n",
    "            scores.append(score)\n",
    "            print('{}, {:.3f}'.format(obj, score))\n",
    "\n",
    "        (x1, y1, x2, y2) = out[i]['rois'][0].astype(np.int)\n",
    "        cv2.rectangle(ori_imgs[i], (x1, y1), (x2, y2), (255, 255, 0), 1)\n",
    "\n",
    "        cv2.putText(ori_imgs[i], '{}, {:.3f}'.format(objs[0], scores[0]),\n",
    "                    (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
    "                    (255, 255, 0), 1)\n",
    "        plt.imshow(ori_imgs[i])\n",
    "        # image saving\n",
    "        cv2.imwrite('test/asl_output_{}.png'.format(idx+1), ori_imgs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Inference with Live Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.backends import cudnn\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess, preprocess_video\n",
    "\n",
    "# Video's path\n",
    "# use gstreamer for video directly; set the fps\n",
    "video_src ='v4l2src device=/dev/video0 ! video/x-raw,framerate=30/1 ! videoconvert ! video/x-raw, format=BGR ! appsink'\n",
    "#video_src = 0\n",
    "\n",
    "compound_coef = 0\n",
    "force_input_size = None  # set None to use default size\n",
    "\n",
    "threshold = 0.2\n",
    "iou_threshold = 0.2\n",
    "\n",
    "use_cuda = True\n",
    "use_float16 = False\n",
    "cudnn.fastest = True\n",
    "cudnn.benchmark = True\n",
    "\n",
    "obj_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
    "            'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'four', \n",
    "            'hello', 'help', 'one', 'right', 'thanks', 'three', 'two', 'zero' ]\n",
    "\n",
    "# tf bilinear interpolation is different from any other's, just make do\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "input_size = input_sizes[compound_coef] if force_input_size is None else force_input_size\n",
    "\n",
    "# load model\n",
    "model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(obj_list))\n",
    "model.load_state_dict(torch.load('logs/asl/'+weight_file[-1]))\n",
    "model.requires_grad_(False)\n",
    "model.eval()\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "if use_float16:\n",
    "    model = model.half()\n",
    "\n",
    "# function for display\n",
    "def display(preds, imgs):\n",
    "    for i in range(len(imgs)):\n",
    "        if len(preds[i]['rois']) == 0:\n",
    "            return imgs[i]\n",
    "\n",
    "        scores = []\n",
    "        objs = []\n",
    "        for j in range(len(preds[i]['rois'])):\n",
    "            (x1, y1, x2, y2) = preds[i]['rois'][j].astype(np.int)\n",
    "            cv2.rectangle(imgs[i], (x1, y1), (x2, y2), (255, 255, 0), 1)\n",
    "            obj = obj_list[preds[i]['class_ids'][j]]\n",
    "            score = float(preds[i]['scores'][j])\n",
    "            objs.append(obj)\n",
    "            scores.append(score)\n",
    "\n",
    "            cv2.putText(imgs[i], '{}, {:.3f}'.format(objs[0], scores[0]),\n",
    "                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
    "                        (255, 255, 0), 1)\n",
    "        \n",
    "        return imgs[i]\n",
    "# Box\n",
    "regressBoxes = BBoxTransform()\n",
    "clipBoxes = ClipBoxes()\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(video_src)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # frame preprocessing\n",
    "    ori_imgs, framed_imgs, framed_metas = preprocess_video(frame, max_size=input_size)\n",
    "\n",
    "    if use_cuda:\n",
    "        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "    else:\n",
    "        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "    # model predict\n",
    "    with torch.no_grad():\n",
    "        features, regression, classification, anchors = model(x)\n",
    "\n",
    "        out = postprocess(x,\n",
    "                        anchors, regression, classification,\n",
    "                        regressBoxes, clipBoxes,\n",
    "                        threshold, iou_threshold)\n",
    "\n",
    "    # result\n",
    "    out = invert_affine(framed_metas, out)\n",
    "    img_show = display(out, ori_imgs)\n",
    "\n",
    "    # show frame by frame\n",
    "    cv2.imshow('frame',img_show)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train_shape.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
